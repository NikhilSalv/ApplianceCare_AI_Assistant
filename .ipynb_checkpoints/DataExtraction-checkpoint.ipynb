{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16503c90-eb80-439e-af73-02520a2afa7d",
   "metadata": {},
   "source": [
    "Extract Document Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53740d77-fb19-4727-90f2-2cd75894c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymupdf openai tiktoken python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2994ae7-101f-45fd-ae7c-149d81e75a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: All about repairing small appliances.pdf\n",
      "  -> Extracted 244,610 chars -> 154,095 chars cleaned -> Saved: All about repairing small appliances.txt\n",
      "Processing: Care and repair of your large home appliances.pdf\n",
      "  -> Extracted 138,342 chars -> 100,813 chars cleaned -> Saved: Care and repair of your large home appliances.txt\n",
      "Processing: All about repairing major household appliances.pdf\n",
      "  -> Extracted 144,034 chars -> 129,411 chars cleaned -> Saved: All about repairing major household appliances.txt\n",
      "Processing: All thumbs guide to VCRs.pdf\n",
      "  -> Extracted 104,793 chars -> 89,572 chars cleaned -> Saved: All thumbs guide to VCRs.txt\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"Extract clean text from PDF, handles images/tables automatically\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    \n",
    "    for page in doc:\n",
    "        # Extract text blocks (ignores images, keeps readable content)\n",
    "        page_text = page.get_text(\"text\")\n",
    "        text.append(page_text.strip())\n",
    "    \n",
    "    doc.close()\n",
    "    return \"\\n\\n\".join(text)\n",
    "\n",
    "def clean_text(raw_text: str) -> str:\n",
    "    \"\"\"Aggressive OCR noise removal + semantic cleanup\"\"\"\n",
    "    \n",
    "    # Step 1: Remove heavy OCR garbage (symbols, random letters, prices)\n",
    "    text = re.sub(r'[$€£¥]\\d+\\.?\\d*|\\b[kx]+|\\b[vj]+|\\b[L»]+\\b|[•/\\\\|]+|[%#*®]+|[-]{2,}', ' ', raw_text)\n",
    "    text = re.sub(r'\\b\\w{1,2}[^a-zA-Z\\s]{1,2}\\b', ' ', text)  # Short gibberish\n",
    "    \n",
    "    # Step 2: Remove common OCR artifacts (CIVIV/M, OMNIVM, etc.)\n",
    "    text = re.sub(r'\\b[A-Z]{2,}M\\b|\\bfoMccui\\b|\\bM®e®\\b|\\bL[V^]+\\b', ' ', text)\n",
    "    text = re.sub(r'\\b\\w+wm\\)\\b|\\bzxwm\\)\\b', ' ', text)\n",
    "    \n",
    "    # Step 3: Normalize whitespace aggressively\n",
    "    text = re.sub(r'\\s*\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'\\n[ \\t]*\\n', '\\n\\n', text)  # Clean single newlines\n",
    "    \n",
    "    # Step 4: Remove lines with mostly noise (short lines, all caps fragments)\n",
    "    lines = text.split('\\n')\n",
    "    clean_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if len(line) > 20 and not re.match(r'^[A-Z\\s\\W]+$', line):  # Skip short/noisy lines\n",
    "            clean_lines.append(line)\n",
    "    \n",
    "    text = '\\n\\n'.join(clean_lines)\n",
    "    \n",
    "    # Step 5: Final polish - remove excessive spaces, strip\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Step 6: Remove common publisher fragments\n",
    "    text = re.sub(r'boston PUBLIC LIBRARY\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'HAWTHORN BOOKS INC?\\.?\\b.*?(?=Copyright|$)', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"Process all PDFs in folder, clean text, save as text files in Extracted_text folder\"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    extracted_folder = folder / \"Extracted_text\"\n",
    "    extracted_folder.mkdir(exist_ok=True)  # Create folder if it doesn't exist\n",
    "    docs = []\n",
    "    \n",
    "    for pdf_file in folder.glob(\"*.pdf\"):\n",
    "        print(f\"Processing: {pdf_file.name}\")\n",
    "        \n",
    "        # Extract raw text\n",
    "        raw_text = extract_pdf_text(pdf_file)\n",
    "        \n",
    "        # Clean the text\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        \n",
    "        # Save individual text file to Extracted_text folder\n",
    "        txt_path = extracted_folder / f\"{pdf_file.stem}.txt\"\n",
    "        txt_path.write_text(cleaned_text)\n",
    "        \n",
    "        docs.append({\n",
    "            \"source\": pdf_file.name,\n",
    "            \"text\": cleaned_text,  # Store cleaned version\n",
    "            \"chunk_id\": f\"{pdf_file.stem}_full\"\n",
    "        })\n",
    "        \n",
    "        print(f\"  -> Extracted {len(raw_text):,} chars -> {len(cleaned_text):,} chars cleaned -> Saved: {txt_path.name}\")\n",
    "    \n",
    "    return docs\n",
    "\n",
    "\n",
    "# Usage\n",
    "pdf_folder = \"PDFs\"\n",
    "documents = process_folder(pdf_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7da81b9-c6ae-41e7-a2b1-5e9a6ccf4667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Illustrated by Carl Bryant W. Clement Stone, Publisher s nice, of course, to have a handyman around the house, but never there when the steam steam or the toaster won’t is written for and dedicated to the housewife who needs a repair manual on ast array of small electrical appliances. Edited by the Staff of Vocational Horizons, Inc. rights reserved, including the right to reproduce this book or portions thereof in any form, except inquiries should be addressed Madison Avenue, New York, New York of America and published taneously in Canada by Prentice-Hall of Canada, Limited, Birch- mount Road, Scarborough, Ontario. Library of Congress Catalog Card I: Repair Problems and Their Solutions Iron does not slide smoothly over cloth Not enough steam comes from the iron Water and steam sputters out of steam ports during operation Iron does not spray water Iron does not shut off Toaster does not heat One side of bread does not toast Bread does not stay down Toaster does not pop up Bread toasts too light or too dark User gets an electric shock when metal shell of toaster is touched Heating element does not come on Fuse blows or circuit breaker trips when broiler is turned on Fuse blows or circuit breaker trips when broiler is plugged into wall outlet with switch in the “off” position Coffee has poor flavor Water boils but does not enter coffee basket in percolator-type is too weak or too strong for desired setting Lid falls off when user pours coffee Indicator lamp does not operate House lights dim or fuse blows when aporizer Vaporizer produces little or no steam Appliance does not heat Pan-fried food sticks and or burns at prescribed settings Waffles stick to grill No heat on one or both waffle grills Plug becomes excessively hot Heating element glows red, but not enough heat can be felt The heater does not heat Does not pick up lint from rug Broom has weak suction and burning odor Motor runs but one or both beaters do not turn Excessive noise during operation Motor stops and'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0][\"text\"][:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c037a6e4-c29b-424f-a2a0-14bbbc0b0c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "340ea364-a230-4e58-8f7a-c3a907bfb2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source', 'text', 'chunk_id'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e925f5-5a37-47ba-a809-231d3c0fb656",
   "metadata": {},
   "source": [
    "Create Document Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad07261-8025-436b-8ed1-9a39c77928e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "splits = text_splitter.split_documents(eur_lex_docs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b7c7eb2-0128-4143-a076-e2f7d769d66f",
   "metadata": {},
   "source": [
    "Create embeddings and store in PinecodeDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "881c3ed9-7636-431f-be18-fa6b6c13d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pinecone\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ddac948f-1666-453a-a946-5d461e0f4330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone key loaded: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# This looks for a .env file in the current directory or parent directories\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "print(\"Pinecone key loaded:\", bool(PINECONE_API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f218ebf-a556-42e7-84be-43ebd0e6697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"]\n",
    ")\n",
    "\n",
    "# # 2️⃣ Create index if it doesn't exist\n",
    "# if \"cra-index\" not in pc.list_indexes().names():\n",
    "#     pc.create_index(\n",
    "#         name=\"appliance-care-data\",\n",
    "#         dimension=512,          # must match embedding size\n",
    "#         metric=\"cosine\",\n",
    "#         spec=ServerlessSpec(\n",
    "#             cloud=\"aws\",\n",
    "#             region=\"us-east-1\"   # pick your cloud region\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ecc6c33-d192-4e00-8ba7-cfc10b855295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deletion_protection': 'disabled',\n",
      " 'dimension': 512,\n",
      " 'embed': {'dimension': 512,\n",
      "           'field_map': {'text': 'text'},\n",
      "           'metric': 'cosine',\n",
      "           'model': 'llama-text-embed-v2',\n",
      "           'read_parameters': {'dimension': 512.0,\n",
      "                               'input_type': 'query',\n",
      "                               'truncate': 'END'},\n",
      "           'vector_type': 'dense',\n",
      "           'write_parameters': {'dimension': 512.0,\n",
      "                                'input_type': 'passage',\n",
      "                                'truncate': 'END'}},\n",
      " 'host': 'appliance-care-data-z61yr43.svc.aped-4627-b74a.pinecone.io',\n",
      " 'metric': 'cosine',\n",
      " 'name': 'appliance-care-data',\n",
      " 'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
      " 'status': {'ready': True, 'state': 'Ready'},\n",
      " 'tags': None,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "index_info = pc.describe_index(\"appliance-care-data\")\n",
    "print(index_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc67608b-c836-4c89-9f4e-b187a5f08def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openai api key loaded: False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file from current or parent directory\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize clients\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index = pc.Index(\"appliance-care-data\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"Openai api key loaded:\", bool(OPENAI_API_KEY))\n",
    "\n",
    "print(OPENAI_API_KEY)\n",
    "# def embed_and_upsert(documents, batch_size=100):\n",
    "#     \"\"\"Chunk, embed with OpenAI, upsert to Pinecone\"\"\"\n",
    "#     all_vectors = []\n",
    "    \n",
    "#     for doc in documents:\n",
    "#         # Chunk the text\n",
    "#         chunks = chunk_text(doc[\"text\"])\n",
    "        \n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             # Generate embedding\n",
    "#             response = openai.embeddings.create(\n",
    "#                 input=chunk,\n",
    "#                 model=\"text-embedding-3-small\"  # or text-embedding-3-large\n",
    "#             )\n",
    "#             embedding = response.data[0].embedding\n",
    "            \n",
    "#             # Prepare Pinecone record\n",
    "#             vector = {\n",
    "#                 \"id\": f\"{doc['chunk_id']}_{i+1}\",\n",
    "#                 \"values\": embedding,\n",
    "#                 \"metadata\": {\n",
    "#                     \"source\": doc[\"source\"],\n",
    "#                     \"text\": chunk[:1000],  # Truncate for metadata limits\n",
    "#                     \"chunk_index\": i+1,\n",
    "#                     \"total_chunks\": len(chunks)\n",
    "#                 }\n",
    "#             }\n",
    "#             all_vectors.append(vector)\n",
    "    \n",
    "#     # Upsert in batches\n",
    "#     for i in range(0, len(all_vectors), batch_size):\n",
    "#         batch = all_vectors[i:i+batch_size]\n",
    "#         index.upsert(vectors=batch)\n",
    "#         print(f\"Upserted batch {i//batch_size + 1} ({len(batch)} vectors)\")\n",
    "    \n",
    "#     print(f\"✅ Populated {len(all_vectors)} vectors to Pinecone\")\n",
    "\n",
    "# # Run it\n",
    "# embed_and_upsert(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f75ad4-3cc3-405c-9ea5-749e9ce376c7",
   "metadata": {},
   "source": [
    "Test Basic Search functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747d622-788b-42b0-9b2e-37d60970e7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71134687-af27-4046-b1ae-d228dcca3a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
